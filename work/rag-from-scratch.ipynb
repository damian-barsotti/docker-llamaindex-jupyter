{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645b1fa1-5ab0-4b4a-aafa-7c46aaf8733e",
   "metadata": {},
   "source": [
    "# [Building RAG from Scratch](https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46dfedb-dc2e-495c-b85c-42a44810ffd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ab8f4-542c-4cf3-9d33-d7390608d370",
   "metadata": {},
   "source": [
    "### Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001d37a-8aac-4397-abc0-739fec0112cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f04d42-c0cc-4fe9-88af-1a2a0bf2f762",
   "metadata": {},
   "source": [
    "### Llama CPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6b79a-081c-4160-8a27-4203876aed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "# model_url = \"https://huggingface.co/afrideva/stablelm-2-1_6b-GGUF/resolve/main/stablelm-2-1_6b.fp16.gguf\"\n",
    "model_url = \"https://huggingface.co/3Simplex/Meta-Llama-3.1-8B-Instruct-gguf/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf\"\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    # model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca515da-8f4e-4197-95a2-3b3a12aeba51",
   "metadata": {},
   "source": [
    "### Initialize Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00424e55-d05e-4ec7-9b56-4b30132e9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"postgres_db\"\n",
    "password = \"postgres\"\n",
    "port = \"5435\"\n",
    "user = \"postgres\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d992aef-0b2a-4e44-b5a7-4b546192ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf7999-b938-4942-9000-86b1b054e29c",
   "metadata": {},
   "source": [
    "## Build an Ingestion Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ae311-2e7b-4936-9100-20e0187f0ba6",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "```sh\n",
    "mkdir data\n",
    "wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3ef98-95df-4f6f-b1f8-f5a262910df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae05339-6755-4e79-a3ec-7539e5e3e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460b39f-d2b3-4314-b24a-9cc593435794",
   "metadata": {},
   "source": [
    "### 2. Use a Text Splitter to Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913e25d-b2a3-4b4d-9a9e-bb32122c5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ae043-6fbe-4219-9e13-a2aadb21838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_chunks))\n",
    "print(text_chunks[0])\n",
    "print(text_chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8bce6e-1099-4f27-8bfc-43b6297d14e2",
   "metadata": {},
   "source": [
    "### 3. Manually Construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e0917-bca8-4fbb-82ce-d6acc46b7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a02289-0a3b-479b-8d35-786dea597a31",
   "metadata": {},
   "source": [
    "### 4. Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104c823-2928-4d09-81b5-f71da8526951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7e2b5-5aaf-4afd-9fb4-2f9a599a021c",
   "metadata": {},
   "source": [
    "### 5. Load Nodes into a Vector Store (Postgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3506b5-5e22-4c02-bf7b-267e4e18afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994692a-f626-4866-abb8-6d62303d1716",
   "metadata": {},
   "source": [
    "## Build Retrieval Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b0b5e-8abd-4442-8461-ae9fbdc6d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de2f93-9608-490f-8f91-bb887a74583b",
   "metadata": {},
   "source": [
    "### 1. Generate a Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd5c5d-c72b-4edf-9047-6d12d34b0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1eff7-a78d-46a4-9cd0-15255b7149b3",
   "metadata": {},
   "source": [
    "### 2. Query the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a25e4-4e82-4962-be47-a4cf4552a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ee5d-cfcf-44d8-9a6a-cf813f60f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f2871-f590-4d32-ab2e-6984ad56476f",
   "metadata": {},
   "source": [
    "### 3. Parse Result into a Set of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d1349-1c66-4c39-a5b1-b5f5794a2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a07e3a-1f09-4325-8f41-a78e5e659a65",
   "metadata": {},
   "source": [
    "### 4. Put into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc31be6-8505-4326-b619-04250c8b3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d28cea-d5d4-4abf-b461-9d577e2cee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f38c09-0c3a-4184-a4e6-375e779c96d0",
   "metadata": {},
   "source": [
    "## Plug this into our RetrieverQueryEngine to synthesize a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ca8c4-704a-4db8-a58f-6443717a2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a858e5c-fd81-405f-9e9d-b9aa5b173ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "\n",
    "response = query_engine.query(query_str)\n",
    "# Total time llama 3.1: 180 seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40065706-dec7-4f36-b54c-9dad7d0be383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad94279-474c-45ef-a311-f36542224cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc6800-b95a-4a3d-a4eb-2952df862538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
